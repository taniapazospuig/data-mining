{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Session 08: Data streams\n",
    "\n",
    "We will take a large corpus of documents and compute some statistics using data streams methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: TÃ nia Pazos\n",
    "\n",
    "E-mail: tania.pazos01@estudiant.upf.edu\n",
    "\n",
    "Date: 20/11/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import random\n",
    "import statistics\n",
    "import secrets\n",
    "import re\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset and how to iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input file contain lines of dialogue of a set of movies from the [Movie Dialog Corpus](https://www.kaggle.com/datasets/Cornell-University/movie-dialog-corpus). We will use the file `movie_lines.tsv` which contains the text of the dialogue, about 3 million words in about 300,000 lines of dialogue.\n",
    "\n",
    "During this practice, we will never load this file in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "INPUT_FILE = \"data/movie_lines.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "POS_NOUN = 'NN'\n",
    "POS_VERB = 'VB'\n",
    "POS_ADJECTIVE = 'JJ'\n",
    "\n",
    "# Producer in Python that reads a file by words that are nouns\n",
    "def read_by_parts_of_speech(filename, parts_of_speech, max_words=-1, report_every=-1):\n",
    "    \n",
    "    # Open the input file\n",
    "    with gzip.open(INPUT_FILE, \"rt\", encoding='utf8') as file:\n",
    "        \n",
    "        # Initialize counter of words to stop at max_words\n",
    "        counter = 0\n",
    "    \n",
    "        # Iterate through lines in the file\n",
    "        for line in file:\n",
    "            \n",
    "            elements = line.split(\"\\t\")\n",
    "            \n",
    "            text = \"\"\n",
    "            if len(elements) >= 5:\n",
    "                text = elements[4].strip()\n",
    "                                        \n",
    "            if counter > max_words and max_words != -1:\n",
    "                break\n",
    "                \n",
    "            for sentence in nltk.sent_tokenize(text):\n",
    "                \n",
    "                tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "                for word in [part[0] for part in tagged if part[1] in parts_of_speech]:\n",
    "                \n",
    "                    counter += 1\n",
    "\n",
    "                    # Report\n",
    "                    if (report_every != -1) and (counter % report_every == 0):\n",
    "                        if max_words == -1:\n",
    "                            print(\"- Read %d words so far\" % (counter))\n",
    "                        else:\n",
    "                            print(\"- Read %d/%d words so far\" % (counter, max_words))\n",
    "\n",
    "                    # Produce the word in lowercase\n",
    "                    yield word.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a first pass over the data. Here we will read only the first 30K adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/taniapazospuig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/taniapazospuig/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current noun 'amazin'\n",
      "Current noun 'sick'\n",
      "Current noun 'fine'\n",
      "Current noun 'much'\n",
      "Current noun 'mannish'\n",
      "Current noun 'second'\n",
      "Current noun 'pro'\n",
      "Current noun 'dramatic'\n",
      "Current noun 'bad'\n",
      "Current noun 'sick'\n",
      "Current noun 'real'\n",
      "- Read 10000/30000 words so far\n",
      "Current noun 'good'\n",
      "Current noun 'own'\n",
      "Current noun 'long'\n",
      "Current noun 'fucking'\n",
      "Current noun 'ta'\n",
      "Current noun 'little'\n",
      "Current noun 'sixteen'\n",
      "Current noun 'american'\n",
      "Current noun 'full'\n",
      "Current noun 'current'\n",
      "Current noun 'nice'\n",
      "- Read 20000/30000 words so far\n",
      "Current noun 'new'\n",
      "Current noun 'serious'\n",
      "Current noun 'good'\n",
      "Current noun 'certain'\n",
      "Current noun 'sorry'\n",
      "Current noun 'evil'\n",
      "Current noun 'double-'\n",
      "Current noun 'ready'\n",
      "Current noun 'pretty'\n",
      "Current noun 'little'\n",
      "Current noun 'many'\n",
      "Current noun 'little'\n",
      "Current noun 'worth'\n",
      "Current noun 'uh'\n",
      "Current noun 'interesting'\n",
      "Current noun 'american'\n",
      "Current noun 'short'\n",
      "Current noun 'fucking'\n",
      "Current noun 'cockamamie'\n",
      "Current noun 'last'\n",
      "Current noun 'chief'\n",
      "Current noun 'southern'\n",
      "Current noun 'fifty'\n",
      "- Read 30000/30000 words so far\n"
     ]
    }
   ],
   "source": [
    "for word in read_by_parts_of_speech(INPUT_FILE, [POS_ADJECTIVE], max_words=30000, report_every=10000):\n",
    "    # Prints 1/1000 of words\n",
    "    if random.random() < 0.001:\n",
    "        print(\"Current adjective '%s'\" % (word)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine approximately the top-10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of loading the entire dataset in main memory, we will use reservoir sampling to determine approximately the top-10 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function uses reservoir sampling to maintain a random sample of fixed size S from a stream of data where the total number of items N is unknown or very large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    # If the reservoir is full, evict an old random item first\n",
    "    if len(reservoir) >= max_reservoir_size:\n",
    "        index_to_replace = random.randint(0, len(reservoir) - 1)\n",
    "        reservoir[index_to_replace] = item\n",
    "    else:\n",
    "        # If the reservoir is not full, just add the new item\n",
    "        reservoir.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `reservoir_sampling` iterates through a file using the reservoir sampling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(filename, parts_of_speech, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "    words_read = 0\n",
    "    \n",
    "    # Iterate through words from the file\n",
    "    for word in read_by_parts_of_speech(filename, parts_of_speech, max_words=max_words, report_every=report_every):\n",
    "        if max_words != -1 and words_read >= max_words:\n",
    "            break  # Stop if we have read more words than max_words\n",
    "            \n",
    "        words_read += 1\n",
    "        \n",
    "        if len(reservoir) < reservoir_size:\n",
    "            # Reservoir is not full, just add the word\n",
    "            add_to_reservoir(reservoir, word, reservoir_size)\n",
    "        else:\n",
    "            # Reservoir is full, decide whether to evict a word\n",
    "            # With probability reservoir_size /words_read, add the word to the reservoir\n",
    "            if random.random() < reservoir_size / words_read:\n",
    "                add_to_reservoir(reservoir, word, reservoir_size)\n",
    "                \n",
    "         \n",
    "        # Report progress every report_every words\n",
    "        if report_every != -1 and words_read % report_every == 0:\n",
    "            if max_words == -1:\n",
    "                print(f\"- Read {words_read} words so far\")\n",
    "            else:\n",
    "                print(f\"- Read {words_read}/{max_words} words so far\")\n",
    "            \n",
    "    return words_read, reservoir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/30000 words so far\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Number of items seen    : 30000\n",
      "Number of items sampled : 1500\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "reservoir_size = 1500\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=30000, report_every=10000)\n",
    "\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reservoir contains repeated items. We compute the absolute frequencies of the top 20 words with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 good\n",
      "38 little\n",
      "31 other\n",
      "29 much\n",
      "29 last\n",
      "29 big\n",
      "22 sorry\n",
      "22 right\n",
      "22 first\n",
      "17 only\n",
      "17 dead\n",
      "17 bad\n",
      "16 young\n",
      "15 wrong\n",
      "15 same\n",
      "15 real\n",
      "15 next\n",
      "14 whole\n",
      "14 sure\n",
      "14 nice\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:20]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"%d %s\" % (absolute_frequency, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the relative frequencies of the top 20 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.07% good\n",
      "2.53% little\n",
      "2.07% other\n",
      "1.93% much\n",
      "1.93% last\n",
      "1.93% big\n",
      "1.47% sorry\n",
      "1.47% right\n",
      "1.47% first\n",
      "1.13% only\n",
      "1.13% dead\n",
      "1.13% bad\n",
      "1.07% young\n",
      "1.00% wrong\n",
      "1.00% same\n",
      "1.00% real\n",
      "1.00% next\n",
      "0.93% whole\n",
      "0.93% sure\n",
      "0.93% nice\n"
     ]
    }
   ],
   "source": [
    "total_items = len(reservoir)\n",
    "\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    relative_frequency = (absolute_frequency / total_items) * 100  # Convert to percentage\n",
    "    print(f\"{relative_frequency:.2f}% {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we see an item C times in the reservoir, we can estimate the item appears *C x dataset_size / reservoir_size* times in the entire dataset (*dataset_size* is the size of the entire dataset). \n",
    "\n",
    "The code below lists the top-5 words and the estimate of their relative and aboslute frequency in the entire dataset for various sizes of the reservoir: 50, 100, 500, 1000, 1500, and 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_frequency_in_dataset(item_count_in_reservoir, reservoir_size, dataset_size):\n",
    "    return item_count_in_reservoir * dataset_size / reservoir_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing reservoir size: 50\n",
      "Relative Frequency in dataset: 10.00% | Absolute Frequency in dataset: 300000 | Word: good\n",
      "Relative Frequency in dataset: 6.00% | Absolute Frequency in dataset: 180000 | Word: true\n",
      "Relative Frequency in dataset: 4.00% | Absolute Frequency in dataset: 120000 | Word: old\n",
      "Relative Frequency in dataset: 4.00% | Absolute Frequency in dataset: 120000 | Word: little\n",
      "Relative Frequency in dataset: 4.00% | Absolute Frequency in dataset: 120000 | Word: late\n",
      "\n",
      "Testing reservoir size: 100\n",
      "Relative Frequency in dataset: 3.00% | Absolute Frequency in dataset: 90000 | Word: u\n",
      "Relative Frequency in dataset: 3.00% | Absolute Frequency in dataset: 90000 | Word: sure\n",
      "Relative Frequency in dataset: 3.00% | Absolute Frequency in dataset: 90000 | Word: only\n",
      "Relative Frequency in dataset: 3.00% | Absolute Frequency in dataset: 90000 | Word: good\n",
      "Relative Frequency in dataset: 3.00% | Absolute Frequency in dataset: 90000 | Word: few\n",
      "\n",
      "Testing reservoir size: 500\n",
      "Relative Frequency in dataset: 4.20% | Absolute Frequency in dataset: 126000 | Word: right\n",
      "Relative Frequency in dataset: 4.20% | Absolute Frequency in dataset: 126000 | Word: good\n",
      "Relative Frequency in dataset: 2.40% | Absolute Frequency in dataset: 72000 | Word: old\n",
      "Relative Frequency in dataset: 2.00% | Absolute Frequency in dataset: 60000 | Word: u\n",
      "Relative Frequency in dataset: 2.00% | Absolute Frequency in dataset: 60000 | Word: next\n",
      "\n",
      "Testing reservoir size: 1000\n",
      "Relative Frequency in dataset: 4.10% | Absolute Frequency in dataset: 123000 | Word: good\n",
      "Relative Frequency in dataset: 2.20% | Absolute Frequency in dataset: 66000 | Word: right\n",
      "Relative Frequency in dataset: 2.10% | Absolute Frequency in dataset: 63000 | Word: little\n",
      "Relative Frequency in dataset: 1.80% | Absolute Frequency in dataset: 54000 | Word: sorry\n",
      "Relative Frequency in dataset: 1.80% | Absolute Frequency in dataset: 54000 | Word: great\n",
      "\n",
      "Testing reservoir size: 1500\n",
      "Relative Frequency in dataset: 3.80% | Absolute Frequency in dataset: 114000 | Word: little\n",
      "Relative Frequency in dataset: 3.80% | Absolute Frequency in dataset: 114000 | Word: good\n",
      "Relative Frequency in dataset: 2.27% | Absolute Frequency in dataset: 68000 | Word: right\n",
      "Relative Frequency in dataset: 2.00% | Absolute Frequency in dataset: 60000 | Word: u\n",
      "Relative Frequency in dataset: 1.73% | Absolute Frequency in dataset: 52000 | Word: other\n",
      "\n",
      "Testing reservoir size: 2000\n",
      "Relative Frequency in dataset: 4.90% | Absolute Frequency in dataset: 147000 | Word: good\n",
      "Relative Frequency in dataset: 2.85% | Absolute Frequency in dataset: 85500 | Word: little\n",
      "Relative Frequency in dataset: 1.85% | Absolute Frequency in dataset: 55500 | Word: right\n",
      "Relative Frequency in dataset: 1.80% | Absolute Frequency in dataset: 54000 | Word: sure\n",
      "Relative Frequency in dataset: 1.80% | Absolute Frequency in dataset: 54000 | Word: other\n"
     ]
    }
   ],
   "source": [
    "# Range of reservoir sizes to test\n",
    "reservoir_sizes = [50, 100, 500, 1000, 1500, 2000]\n",
    "max_words = 200000\n",
    "dataset_size = 3000000 # Assume 3 million words in the dataset\n",
    "\n",
    "for reservoir_size in reservoir_sizes:\n",
    "    # Run reservoir sampling with the given reservoir size\n",
    "    print(f\"\\nTesting reservoir size: {reservoir_size}\")\n",
    "    \n",
    "    items_seen, reservoir = reservoir_sampling(INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=max_words)\n",
    "    \n",
    "    # Compute absolute frequencies in the reservoir\n",
    "    freq = {}\n",
    "    for item in reservoir:\n",
    "        freq[item] = reservoir.count(item)\n",
    "\n",
    "    # Sort items by frequency in descending order and get the top 5\n",
    "    most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:5]\n",
    "    \n",
    "    # Print the estimated frequencies for the top-5 words in the entire dataset\n",
    "    for absolute_frequency, word in most_frequent_items:\n",
    "        estimated_frequency = estimate_frequency_in_dataset(absolute_frequency, reservoir_size, dataset_size)\n",
    "        relative_frequency = (estimated_frequency / dataset_size) * 100  # Convert to percentage\n",
    "        \n",
    "        # Print only the estimates for the entire dataset\n",
    "        print(f\"Relative Frequency in dataset: {relative_frequency:.2f}% | Absolute Frequency in dataset: {estimated_frequency:.0f} | Word: {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing several reservoir sizes, it can be seen that larger reservoir sizes show more stable estimates of word frequencies for the entire dataset. Indeed, smaller reservoir sizes (50 and 100) significantly vary in the top-5 words and their frequencies across runs. However, larger reservoirs (500, 1000, 1500 and 2000) yield more consistent results, with several words appearing consistently in the top-5 list across multiple runs. \n",
    "\n",
    "Based on this analysis, a reservoir size of 1500 is recommended, as it strikes a good balance between computational efficiency and result stability. The variability in the top-5 words and their frequency estimates across multiple runs for a reservoir size of 1500 is minimal, making it a reliable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine approximately the distinct number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will estimate the number of distinct words without creating a dictionary or hash table, but instead, we will use the Flajolet-Martin probabilistic counting method. The Flajolet-Martin probabilistic counting method works as follows:\n",
    "\n",
    "* For several passes\n",
    "   * Create hash funcion h\n",
    "   * For every element *u* in the stream:\n",
    "      * Compute hash value *h(u)*\n",
    "      * Let *r(u)* be the number of trailing zeroes in *h(u)*\n",
    "      * Maintain *R* as the maximum value of *r(u)* seen so far\n",
    "   * Add *2<sup>R</sup>* as an estimate for the number of distinct elements *u* seen\n",
    "* The final estimate is the average or the median of the estimates found in each pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below counts the number of trailing zeroes in the binary representation of a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def count_trailing_zeroes(number):\n",
    "    count = 0\n",
    "    while number & 1 == 0:\n",
    "        count += 1\n",
    "        number = number >> 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this function to generate a random hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def random_hash_function():\n",
    "    # We use a cryptographically safe generator for the salt of our hash function\n",
    "    salt = secrets.token_bytes(32)\n",
    "    return lambda string: hash(string + str(salt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to perform `number_of_passes` passes over the entire file and compute the estimate for the number of distinct words on each pass. Note that we will count nouns, verbs, and adjectives. We will set the limit of words read to 100000 to control computing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate on pass 1: 8192 distinct words\n",
      "Estimate on pass 2: 524288 distinct words\n",
      "Estimate on pass 3: 4096 distinct words\n",
      "Estimate on pass 4: 2048 distinct words\n",
      "Estimate on pass 5: 2048 distinct words\n",
      "Estimate on pass 6: 2048 distinct words\n",
      "Estimate on pass 7: 32768 distinct words\n",
      "Estimate on pass 8: 8192 distinct words\n",
      "Estimate on pass 9: 8192 distinct words\n",
      "Estimate on pass 10: 4096 distinct words\n"
     ]
    }
   ],
   "source": [
    "number_of_passes = 10\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # Generate a new hash function for this pass\n",
    "    hash_function = random_hash_function()\n",
    "    \n",
    "    # Maximum number of trailing zeroes observed in this pass\n",
    "    max_trailing_zeroes = 0\n",
    "    \n",
    "    # Read words by parts of speech\n",
    "    for word in read_by_parts_of_speech(INPUT_FILE, [POS_NOUN, POS_VERB, POS_ADJECTIVE], max_words=100000):\n",
    "        hashed_value = hash_function(word)  # Compute hash value of the word\n",
    "        trailing_zeroes = count_trailing_zeroes(hashed_value)  # Count trailing zeroes\n",
    "        max_trailing_zeroes = max(max_trailing_zeroes, trailing_zeroes)  # Update max\n",
    "    \n",
    "    # Calculate the estimate for this pass\n",
    "    estimate = 2 ** max_trailing_zeroes\n",
    "    estimates.append(estimate)\n",
    "    print(f\"Estimate on pass {i + 1}: {estimate} distinct words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Average of estimates: 59596.8\n",
      "* Median  of estimates: 6144.0\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will repeat this process for nouns, adjectives, and verbs separately. We will perform 3 separate runs of 10 passes each. For each pass, we will compute the noun, verb, and adjective estimate for the number of distinct words. After each run, we will compute the average and the median of the estimates.\n",
    "\n",
    "This is done in the code below; it is commented because it takes a few minutes to run. The obtained results are copied in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1 ===\n",
      "Pass 1:\n",
      "  Noun estimate: 32768\n",
      "  Verb estimate: 16384\n",
      "  Adjective estimate: 32768\n",
      "Pass 2:\n",
      "  Noun estimate: 1048576\n",
      "  Verb estimate: 8192\n",
      "  Adjective estimate: 262144\n",
      "Pass 3:\n",
      "  Noun estimate: 65536\n",
      "  Verb estimate: 1024\n",
      "  Adjective estimate: 16384\n",
      "Pass 4:\n",
      "  Noun estimate: 2048\n",
      "  Verb estimate: 2048\n",
      "  Adjective estimate: 16384\n",
      "Pass 5:\n",
      "  Noun estimate: 32768\n",
      "  Verb estimate: 8192\n",
      "  Adjective estimate: 65536\n",
      "Pass 6:\n",
      "  Noun estimate: 4096\n",
      "  Verb estimate: 16384\n",
      "  Adjective estimate: 16384\n",
      "Pass 7:\n",
      "  Noun estimate: 8192\n",
      "  Verb estimate: 32768\n",
      "  Adjective estimate: 8192\n",
      "Pass 8:\n",
      "  Noun estimate: 8192\n",
      "  Verb estimate: 2048\n",
      "  Adjective estimate: 131072\n",
      "Pass 9:\n",
      "  Noun estimate: 8192\n",
      "  Verb estimate: 8192\n",
      "  Adjective estimate: 32768\n",
      "Pass 10:\n",
      "  Noun estimate: 8192\n",
      "  Verb estimate: 1024\n",
      "  Adjective estimate: 2048\n",
      "\n",
      "Run 1 summary:\n",
      "  * Noun estimates: Average = 121856.0, Median = 8192.0\n",
      "  * Verb estimates: Average = 9625.6, Median = 8192.0\n",
      "  * Adjective estimates: Average = 58368.0, Median = 24576.0\n",
      "\n",
      "=== Run 2 ===\n",
      "Pass 1:\n",
      "  Noun estimate: 4096\n",
      "  Verb estimate: 65536\n",
      "  Adjective estimate: 2048\n",
      "Pass 2:\n",
      "  Noun estimate: 2048\n",
      "  Verb estimate: 2048\n",
      "  Adjective estimate: 2048\n",
      "Pass 3:\n",
      "  Noun estimate: 8192\n",
      "  Verb estimate: 4096\n",
      "  Adjective estimate: 8192\n",
      "Pass 4:\n",
      "  Noun estimate: 16384\n",
      "  Verb estimate: 16384\n",
      "  Adjective estimate: 32768\n",
      "Pass 5:\n",
      "  Noun estimate: 16384\n",
      "  Verb estimate: 16384\n",
      "  Adjective estimate: 65536\n",
      "Pass 6:\n",
      "  Noun estimate: 32768\n",
      "  Verb estimate: 16384\n",
      "  Adjective estimate: 32768\n",
      "Pass 7:\n",
      "  Noun estimate: 32768\n",
      "  Verb estimate: 4096\n",
      "  Adjective estimate: 8192\n",
      "Pass 8:\n",
      "  Noun estimate: 32768\n",
      "  Verb estimate: 4096\n",
      "  Adjective estimate: 8192\n",
      "Pass 9:\n",
      "  Noun estimate: 4096\n",
      "  Verb estimate: 4096\n",
      "  Adjective estimate: 65536\n",
      "Pass 10:\n",
      "  Noun estimate: 4096\n",
      "  Verb estimate: 512\n",
      "  Adjective estimate: 16384\n",
      "\n",
      "Run 2 summary:\n",
      "  * Noun estimates: Average = 15360.0, Median = 12288.0\n",
      "  * Verb estimates: Average = 13363.2, Median = 4096.0\n",
      "  * Adjective estimates: Average = 24166.4, Median = 12288.0\n",
      "\n",
      "=== Run 3 ===\n",
      "Pass 1:\n",
      "  Noun estimate: 8192\n",
      "  Verb estimate: 8192\n",
      "  Adjective estimate: 4096\n",
      "Pass 2:\n",
      "  Noun estimate: 8192\n",
      "  Verb estimate: 1024\n",
      "  Adjective estimate: 2048\n",
      "Pass 3:\n",
      "  Noun estimate: 4096\n",
      "  Verb estimate: 131072\n",
      "  Adjective estimate: 4096\n",
      "Pass 4:\n",
      "  Noun estimate: 8192\n",
      "  Verb estimate: 8192\n",
      "  Adjective estimate: 4096\n",
      "Pass 5:\n",
      "  Noun estimate: 8192\n",
      "  Verb estimate: 4096\n",
      "  Adjective estimate: 4096\n",
      "Pass 6:\n",
      "  Noun estimate: 2048\n",
      "  Verb estimate: 8192\n",
      "  Adjective estimate: 4096\n",
      "Pass 7:\n",
      "  Noun estimate: 8192\n",
      "  Verb estimate: 65536\n",
      "  Adjective estimate: 1024\n",
      "Pass 8:\n",
      "  Noun estimate: 4096\n",
      "  Verb estimate: 4096\n",
      "  Adjective estimate: 8192\n",
      "Pass 9:\n",
      "  Noun estimate: 4096\n",
      "  Verb estimate: 4096\n",
      "  Adjective estimate: 16384\n",
      "Pass 10:\n",
      "  Noun estimate: 4096\n",
      "  Verb estimate: 4096\n",
      "  Adjective estimate: 2048\n",
      "\n",
      "Run 3 summary:\n",
      "  * Noun estimates: Average = 5939.2, Median = 6144.0\n",
      "  * Verb estimates: Average = 23859.2, Median = 6144.0\n",
      "  * Adjective estimates: Average = 5017.6, Median = 4096.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "number_of_runs = 3\n",
    "number_of_passes = 10\n",
    "\n",
    "for run in range(number_of_runs):\n",
    "   \n",
    "    estimates_noun_run = []\n",
    "    estimates_verb_run = []\n",
    "    estimates_adjective_run = []\n",
    "\n",
    "    print(f\"\\n=== Run {run + 1} ===\")\n",
    "    \n",
    "    for i in range(number_of_passes):\n",
    "        # Initialize the maximum trailing zeroes\n",
    "        max_trailing_zeroes_noun = 0\n",
    "        max_trailing_zeroes_verb = 0\n",
    "        max_trailing_zeroes_adjective = 0\n",
    "\n",
    "        # Create a new random hash function for each pass\n",
    "        hash_function = random_hash_function()\n",
    "\n",
    "        # Read nouns\n",
    "        for word in read_by_parts_of_speech(INPUT_FILE, [POS_NOUN], max_words=100000):\n",
    "            hashed_value = hash_function(word)\n",
    "            trailing_zeroes = count_trailing_zeroes(hashed_value)\n",
    "            max_trailing_zeroes_noun = max(max_trailing_zeroes_noun, trailing_zeroes)\n",
    "\n",
    "        # Read verbs\n",
    "        for word in read_by_parts_of_speech(INPUT_FILE, [POS_VERB], max_words=100000):\n",
    "            hashed_value = hash_function(word)\n",
    "            trailing_zeroes = count_trailing_zeroes(hashed_value)\n",
    "            max_trailing_zeroes_verb = max(max_trailing_zeroes_verb, trailing_zeroes)\n",
    "\n",
    "        # Read adjectives\n",
    "        for word in read_by_parts_of_speech(INPUT_FILE, [POS_ADJECTIVE], max_words=100000):\n",
    "            hashed_value = hash_function(word)\n",
    "            trailing_zeroes = count_trailing_zeroes(hashed_value)\n",
    "            max_trailing_zeroes_adjective = max(max_trailing_zeroes_adjective, trailing_zeroes)\n",
    "\n",
    "        # Compute estimates for this pass\n",
    "        estimate_noun = 2 ** max_trailing_zeroes_noun\n",
    "        estimate_verb = 2 ** max_trailing_zeroes_verb\n",
    "        estimate_adjective = 2 ** max_trailing_zeroes_adjective\n",
    "\n",
    "        # Append estimates to the lists\n",
    "        estimates_noun_run.append(estimate_noun)\n",
    "        estimates_verb_run.append(estimate_verb)\n",
    "        estimates_adjective_run.append(estimate_adjective)\n",
    "\n",
    "        \n",
    "        print(f\"Pass {i + 1}:\")\n",
    "        print(f\"  Noun estimate: {estimate_noun}\")\n",
    "        print(f\"  Verb estimate: {estimate_verb}\")\n",
    "        print(f\"  Adjective estimate: {estimate_adjective}\")\n",
    "\n",
    "    # Compute the average and median for this run\n",
    "    avg_noun = statistics.mean(estimates_noun_run)\n",
    "    med_noun = statistics.median(estimates_noun_run)\n",
    "\n",
    "    avg_verb = statistics.mean(estimates_verb_run)\n",
    "    med_verb = statistics.median(estimates_verb_run)\n",
    "\n",
    "    avg_adjective = statistics.mean(estimates_adjective_run)\n",
    "    med_adjective = statistics.median(estimates_adjective_run)\n",
    "\n",
    "   \n",
    "    print(f\"\\nRun {run + 1} summary:\")\n",
    "    print(f\"  * Noun estimates: Average = {avg_noun:.1f}, Median = {med_noun:.1f}\")\n",
    "    print(f\"  * Verb estimates: Average = {avg_verb:.1f}, Median = {med_verb:.1f}\")\n",
    "    print(f\"  * Adjective estimates: Average = {avg_adjective:.1f}, Median = {med_adjective:.1f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== Run 1 ===\n",
    "\n",
    "Pass 1:\n",
    "  Noun estimate: 32768\n",
    "  Verb estimate: 16384\n",
    "  Adjective estimate: 32768\n",
    "  \n",
    "Pass 2:\n",
    "  Noun estimate: 1048576\n",
    "  Verb estimate: 8192\n",
    "  Adjective estimate: 262144\n",
    "  \n",
    "Pass 3:\n",
    "  Noun estimate: 65536\n",
    "  Verb estimate: 1024\n",
    "  Adjective estimate: 16384\n",
    "  \n",
    "Pass 4:\n",
    "  Noun estimate: 2048\n",
    "  Verb estimate: 2048\n",
    "  Adjective estimate: 16384\n",
    "  \n",
    "Pass 5:\n",
    "  Noun estimate: 32768\n",
    "  Verb estimate: 8192\n",
    "  Adjective estimate: 65536\n",
    "  \n",
    "Pass 6:\n",
    "  Noun estimate: 4096\n",
    "  Verb estimate: 16384\n",
    "  Adjective estimate: 16384\n",
    "  \n",
    "Pass 7:\n",
    "  Noun estimate: 8192\n",
    "  Verb estimate: 32768\n",
    "  Adjective estimate: 8192\n",
    "  \n",
    "Pass 8:\n",
    "  Noun estimate: 8192\n",
    "  Verb estimate: 2048\n",
    "  Adjective estimate: 131072\n",
    "  \n",
    "Pass 9:\n",
    "  Noun estimate: 8192\n",
    "  Verb estimate: 8192\n",
    "  Adjective estimate: 32768\n",
    "  \n",
    "Pass 10:\n",
    "  Noun estimate: 8192\n",
    "  Verb estimate: 1024\n",
    "  Adjective estimate: 2048\n",
    "\n",
    "Run 1 summary:\n",
    "  * Noun estimates: Average = 121856.0, Median = 8192.0\n",
    "  * Verb estimates: Average = 9625.6, Median = 8192.0\n",
    "  * Adjective estimates: Average = 58368.0, Median = 24576.0\n",
    "\n",
    "=== Run 2 ===\n",
    "\n",
    "Pass 1:\n",
    "  Noun estimate: 4096\n",
    "  Verb estimate: 65536\n",
    "  Adjective estimate: 2048\n",
    "  \n",
    "Pass 2:\n",
    "  Noun estimate: 2048\n",
    "  Verb estimate: 2048\n",
    "  Adjective estimate: 2048\n",
    "  \n",
    "Pass 3:\n",
    "  Noun estimate: 8192\n",
    "  Verb estimate: 4096\n",
    "  Adjective estimate: 8192\n",
    "  \n",
    "Pass 4:\n",
    "  Noun estimate: 16384\n",
    "  Verb estimate: 16384\n",
    "  Adjective estimate: 32768\n",
    "  \n",
    "Pass 5:\n",
    "  Noun estimate: 16384\n",
    "  Verb estimate: 16384\n",
    "  Adjective estimate: 65536\n",
    "  \n",
    "Pass 6:\n",
    "  Noun estimate: 32768\n",
    "  Verb estimate: 16384\n",
    "  Adjective estimate: 32768\n",
    "  \n",
    "Pass 7:\n",
    "  Noun estimate: 32768\n",
    "  Verb estimate: 4096\n",
    "  Adjective estimate: 8192\n",
    "  \n",
    "Pass 8:\n",
    "  Noun estimate: 32768\n",
    "  Verb estimate: 4096\n",
    "  Adjective estimate: 8192\n",
    "  \n",
    "Pass 9:\n",
    "  Noun estimate: 4096\n",
    "  Verb estimate: 4096\n",
    "  Adjective estimate: 65536\n",
    "  \n",
    "Pass 10:\n",
    "  Noun estimate: 4096\n",
    "  Verb estimate: 512\n",
    "  Adjective estimate: 16384\n",
    "\n",
    "Run 2 summary:\n",
    "  * Noun estimates: Average = 15360.0, Median = 12288.0\n",
    "  * Verb estimates: Average = 13363.2, Median = 4096.0\n",
    "  * Adjective estimates: Average = 24166.4, Median = 12288.0\n",
    "\n",
    "=== Run 3 ===\n",
    "\n",
    "Pass 1:\n",
    "  Noun estimate: 8192\n",
    "  Verb estimate: 8192\n",
    "  Adjective estimate: 4096\n",
    "  \n",
    "Pass 2:\n",
    "  Noun estimate: 8192\n",
    "  Verb estimate: 1024\n",
    "  Adjective estimate: 2048\n",
    "  \n",
    "Pass 3:\n",
    "  Noun estimate: 4096\n",
    "  Verb estimate: 131072\n",
    "  Adjective estimate: 4096\n",
    "  \n",
    "Pass 4:\n",
    "  Noun estimate: 8192\n",
    "  Verb estimate: 8192\n",
    "  Adjective estimate: 4096\n",
    "  \n",
    "Pass 5:\n",
    "  Noun estimate: 8192\n",
    "  Verb estimate: 4096\n",
    "  Adjective estimate: 4096\n",
    "  \n",
    "Pass 6:\n",
    "  Noun estimate: 2048\n",
    "  Verb estimate: 8192\n",
    "  Adjective estimate: 4096\n",
    "  \n",
    "Pass 7:\n",
    "  Noun estimate: 8192\n",
    "  Verb estimate: 65536\n",
    "  Adjective estimate: 1024\n",
    "  \n",
    "Pass 8:\n",
    "  Noun estimate: 4096\n",
    "  Verb estimate: 4096\n",
    "  Adjective estimate: 8192\n",
    "  \n",
    "Pass 9:\n",
    "  Noun estimate: 4096\n",
    "  Verb estimate: 4096\n",
    "  Adjective estimate: 16384\n",
    "  \n",
    "Pass 10:\n",
    "  Noun estimate: 4096\n",
    "  Verb estimate: 4096\n",
    "  Adjective estimate: 2048\n",
    "\n",
    "Run 3 summary:\n",
    "  * Noun estimates: Average = 5939.2, Median = 6144.0\n",
    "  * Verb estimates: Average = 23859.2, Median = 6144.0\n",
    "  * Adjective estimates: Average = 5017.6, Median = 4096.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of the three runs, it can be seen that averages are spread out significantly. For instance, if we focus on the noun estimates, the averages range from 15360 in the second run to 121856 in the first run. On the other hand, the median values are more stable across runs. Again in the case of nouns, the median of estimates go from 6144 in the third run to 12288 in the second run. For verb and adjective estimates, the same behaviour is observed. \n",
    "\n",
    "All in all, given that probabilistic counting methods can sometimes produce skewed distributions with extreme values, the median is a more appropriate measure than the average. This can be explained becuse the median is a more robust measure, i.e., it is less affected by outliers or extreme values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
